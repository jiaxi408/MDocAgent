Published as a conference paper at ICLR 2024
Table 9: Ablation study results comparing RAPTOR with a recency-based tree approach
Configuration
Accuracy
RAPTOR + SBERT embeddings + UnifiedQA
56.6%
Recency-based tree + SBERT embeddings + UnifiedQA
55.8%
C
DATASET STATISTICS AND COMPRESSION RATIOS
The average ratio of the summary length to the sum of child node lengths across all datasets is 0.28,
indicating a 72% compression rate. On average, the summary length is 131 tokens, and the average
child node length is 86 tokens. Below are the detailed statistics for all three datasets:
Table 10: Statistics of Average Summary Length and Child Node Length Across Datasets
Dataset
Avg.
Summary
Length
(tokens)
Avg. Child
Node Text
Length
(tokens)
Avg. # of
Child Nodes
Per Parent
Avg.
Compression
Ratio (%)
All Datasets
131
85.6
6.7
.28
QuALITY
124.4
87.9
5.7
.28
NarrativeQA
129.7
85.5
6.8
.27
QASPER
145.9
86.2
5.7
.35
D
SUMMARIZATION PROMPT
Table 11 shows the prompt used for summarization.
Table 11: Prompt for Summarization
Role
Content
system
You are a Summarizing Text Portal
user
Write a summary of the following, including as many key details as
possible: {context}:
E
HALLUCINATION ANALYSIS
To assess the quality and accuracy of the summarizations within our RAPTOR model, we conducted
an analysis focusing on hallucinations in the generated summaries. The summaries were generated
by gpt-3.5-turbo and subsequently annotated to quantify the rates of hallucinations, to examine
whether such inaccuracies propagate to parent nodes, and to evaluate their impact on question-
answering (QA) tasks.
E.1
METHODOLOGY
We randomly sampled 150 nodes across 40 stories and evaluated them for hallucinations. This
sampling strategy provides a broad view of the modelâ€™s performance across different contexts. Each
node was annotated by hand, and determined if it contained a hallucination.
E.2
FINDINGS
Out of the 150 nodes sampled, 4% (6 nodes) contained some form of hallucination. Most commonly,
these hallucinations originated from the model adding minor information possibly from its training
data that was not present in the text being summarized, or from incorrectly extrapolating some
information when creating the summary.
17
