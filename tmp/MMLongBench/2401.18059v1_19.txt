Published as a conference paper at ICLR 2024
significantly impacts downstream tasks. When GPT-4 is provided with RAPTOR’s context, it gen-
erates a detailed answer: “Cinderella finds a happy ending when the Prince searches for the owner
of the lost glass slipper and discovers it belongs to Cinderella. They eventually marry, transform-
ing Cinderella’s life for the better.” In contrast, using DPR’s context, GPT-4 states: “Based on the
given context, it is not possible to determine how Cinderella finds a happy ending, as the text lacks
information about the story’s conclusion.”
The second question we examine is “What is the central theme of the story?”, a thematic question
that requires holistic understanding of the entire text. The text retrieved by RAPTOR and DPR for
this question is shown in Table 13. The text retrieved by RAPTOR contains short descriptions of
all the major parts of the story, whereas the text retrieved by DPR contains detailed descriptions of
a narrow subset of the story. Again, the difference in retrieval mechanisms affects the performance
of GPT-4 when answering the question. Given DPR’s context, it outputs “The central theme of
the story is transformation and the power of inner beauty, as Cinderella, a kind and humble girl, is
magically transformed into a beautiful princess, capturing the attention and admiration of the Prince
and others at the ball.” This answer only takes into account the first portion of the story, up until
Cinderella first meets the prince. In contrast, given RAPTOR’s context, GPT-4 outputs “The central
theme of the story is transformation and overcoming adversity, as Cinderella, with the help of her
Fairy Godmother, transforms from a mistreated and downtrodden girl into a beautiful and confident
young woman who ultimately finds happiness and love with the Prince.” This is a more complete
answer, demonstrating a comprehensive understanding of the story.
This qualitative analysis indicates that RAPTOR outperforms prior retrieval mechanisms because
the information that it retrieves is more relevant and exhaustive, allowing for better performance on
downstream tasks.
We also created a 2600-word story along with questions about its narrative and theme. An excerpt
from the story is present below and the full PDF of this story is linked here. For questions like “What
is the central theme of the story?”, an upper-level node is retrieved which includes the sentence:
“This story is about the power of human connection... inspiring and uplifting each other as they
pursued their passions.” This summary, not explicitly present in the original text, almost directly
answers the question.
Excerpt from ”The Eager Writer”:
”Ethan’s passion for writing had always been a part of him. As a child, he would
often scribble stories and poems in his notebook, and as he grew older, his love
for writing only intensified. His evenings were often spent in the dim light of his
room, typing away at his laptop. He had recently taken a job as a content writer
for an online marketing firm to pay the bills, but his heart still longed for the
world of storytelling. However, like many aspiring writers, he struggled to find a
foothold in the industry. He took a job as a content writer for an online marketing
firm, but it was growing increasingly evident to him that this was not the path he
wanted to pursue. It was during this time that he stumbled upon the Pathways
app. The app offered a platform for people in similar professions to connect and
share knowledge, and he saw it as an opportunity to finally connect with others
who shared his passion for writing. Ethan saw an opportunity to meet others who
shared his passion and could offer guidance and mentorship. He quickly signed
up and was surprised by the number of writers he found on the platform, from
well establish professionals to beginners just starting out in the business.”
H
NARRATIVEQA EVALUATION SCRIPT
We made several modifications to AllenNLP’s evaluation script3 to better fit our evaluation needs:
• Added Smoothing: Smoothing was incorporated to handle cases where BLEU score is
zero, due to no n-gram matches occurring in the reference text. A BLEU score of zero
skews the results, leading to an overly harsh evaluation for rare or novel phrases. By adding
3docs.allennlp.org/models/main/models/rc/tools/narrativeqa/
20
